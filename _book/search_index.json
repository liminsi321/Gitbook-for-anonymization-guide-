[["index.html", "A Practical Guide to Psedunymization and Anonymization of Research Data 1 Preface", " A Practical Guide to Psedunymization and Anonymization of Research Data Alice Nikuze, Deniece Nazareth, Minsi Li 2025-11-03 1 Preface This guide provides practical guidance on two methods of data de-identification: data pseudonymisation and anonymisation. Data de-identification refers to the process of removing or minimising information that can be used to track back to a research participant within a dataset. This guide includes hands-on examples of various techniques for pseudonymising and anonymising different types of research data. The guide is structured as follows: Chapter 1 defines personal data and explains different types of identifiers. Chapter 2 introduces fundamental principles and concepts of data de-identification. It concludes with practical considerations and best practices. Chapter 3 delves into the techniques of pseudonymisation and anonymisation for textual, numerical, audio-visual, and geospatial research data. Additionally, practical examples are included. Chapters 4, 5 and 6 present the resources, glossary, and attribution. Although extensive techniques and examples are presented, please note that this guide does not cover every issue related to pseudonymisation or anonymisation and does not exhaust all types of research data. Furthermore, the de-identification of research data is often complex and requires a case-by-case approach. Therefore, please always consult your data steward or privacy contact person for more information. Disclaimer: The authors cannot be held responsible for any unintended negative consequences resulting from the incompleteness of this guide, or from the misuse or misinterpretation of its content or discrepancies with any other institutions’ guidance. This guide has been written by: Alice Nikuze, University of Twente Deniece Nazareth, University of Twente Minsi Li, University of Twente The authors would like to extend our sincere gratitude to our former colleague, Dr Qian Zhang, for her immense dedication and contribution during the conceptualization phase of this guide. We also wish to express our sincere appreciation to Nestor De la Paz Ruiz for his contribution, whose efforts were instrumental in transforming this guide into a Gitbook. This work is licensed under a Creative Commons Attribution DOI: https://doi.org/10.5281/zenodo.17492062 Show All News Updates "],["introductionintro.html", "2 Introduction{intro} 2.1 personal data in research", " 2 Introduction{intro} In today’s data-driven world, privacy concerns have become paramount. With the increasing collection of personal data in research, researchers bear the responsibility to protect the privacy of the data subjects. More specifically, the General Data Protection Regulation (GDPR) requires researchers affiliated with any institutions within the European Economic Area (EEA) to protect the privacy of research participants when their personal data is processed in research. In this context, researchers should employ all necessary data protection measures, including de-identification techniques, to minimise the risk of tracking back to a data subject. 2.1 personal data in research The term personal data refers to any information that enables people who have access to the data (including you as a researcher) to identify a living person. According to the GDPR, a living person can be identified, directly or indirectly, by means of an identifier such as a name, a citizen identification number, location data, or by other details specific to the person’s physical, physiological, genetic, mental, economic, cultural, or social identity. 2.1.1 Categories of identifiers There are three categories of identifiers: Name Definition Direct identifiers Any piece of information unique to an individual and sufficient on its own to identify that individual, such as a name, a phone number, a picture, security numbers, audio/sound, video, photographs, etc. Strong indirect identifiers Any information that can reveal someone’s identity when combined with other available information. Examples include an address, age, gender, occupation, location, health-related records, ethnic group, etc. Indirect identifiers It is worth noting that some identifiers are particularly considered strong indirect identifiers because they can significantly increase the risk of easily identifying an individual when combined with other information. Examples of strong identifiers include a rare event, a rare disease, an unusual job title, a date of birth, an IP address, etc. See Figure 1 for examples of identifiers in the three main categories. Figure 1. Examples of identifiers typically processed in research There are other types of identifiers that can significantly increase the risks of re-identification. Notable examples are: Biometric identifiers: which include unique and often permanent traits, such as physiological and behavioural characteristics, including facial features, fingerprints, DNA, voice, and gait. Biometric identifiers also include less distinctive traits that may change over time, such as visible marks (e.g., tattoos or scars). Contextual elements: like speaking or writing styles, clothing style and hair style, and specific social or environmental settings can, in some cases, also increase the risks of identification. 2.1.2 Special categories of personal data There are special categories of personal data, also known as sensitive personal data. Sensitive personal data includes any information about a person’s racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health, sex life, or sexual orientation. This personal data can put individuals or particular groups in vulnerable positions, and their misuse can potentially lead to physical or mental harm, financial loss, political or religious persecution, disclosure of sexual orientation, discrimination, or embarrassment. Therefore, handling such data requires extra caution. "],["minimization.html", "3 Principles, concepts and practical considerations 3.1 Data minimization 3.2 Pseudonymisation versus Anonymisation 3.3 Practical Consideration", " 3 Principles, concepts and practical considerations 3.1 Data minimization Data minimization is a fundamental principle of data privacy and protection. This principle requires limiting personal data collection and retention to only what is relevant, necessary, and adequate to only what is to accomplish a given purpose. Collecting only the minimum amount of personal data necessary for your research reduces the risks associated with excessive data collection. More personal data implies higher risks of data breaches and their misuse, thereby increasing potential harm for the research participants. Personal data minimisation in research can be achieved by: Carefully planning what personal data is necessary for your research purpose. Plan what specific personal data is truly needed early in the research design phase. For example, avoid collecting participants’ ages if your study does not examine age-related effects. Collecting aggregated information when detailed information is not required. If broader categories (e.g., age ranges, geographic area) suffice for your analysis, there’s no need to collect exact ages. Limiting open-ended questions in surveys. Open-ended questions can give research participants room to share unsolicited personal data about themselves or others. Reducing the number of open-ended questions can minimise the risks of collecting unnecessary personal data. Instructing research participants sufficiently to avoid unnecessary personal data. Inform participants ahead of the data collection to refrain from sharing unnecessary personal information, such as names or addresses of others. Adding clear instructions and reminding the participants not to mention personal data (e.g., other people’s names or addresses) can prevent unintentional personal data. Adjusting the settings of the data collection tool to minimise personal data collection. Review and adjust the settings of data collection tools like survey platforms to prevent the default collection of unnecessary personal data, such as location or IP addresses. When data collection is finalised, please migrate personal data from the data collection tools to safe data storage as soon as possible. 3.2 Pseudonymisation versus Anonymisation When personal data are necessary and have been collected for research, research participants’ identities should be protected. Two techniques, pseudonymisation and anonymization, are used to ensure that research participants remain anonymous and untraceable during and after research. 3.2.1 Pseudonymisation Pseudonymisation is a de-identification process after which research data can no longer be linked to a specific individual without additional information. This process also referred to as coding, involves creating two separate files: one linking an individual’s identifying information to pseudonyms (substitute values) and another containing only the pseudonyms along with non-identifying research data. The former file is known as a key file or code list and must be kept separately from the latter file. See Figure 2 below for an example of pseudonymisation. Figure 2: Examples of pseudonymisation of names The key file should be securely stored and protected with measures such as password protection and encryption to prevent the re-identification of research participants. Encryption is the process of securing (personal) data by converting its storage location, such as a file, folder, USB device, hard disk drive, into a coded format, making it accessible only to those with the decryption key. 3.2.2 Anonymisation Anonymisation is another de-identification process which involves permanently deleting direct and/or indirect identifiers from the data, such that there is no way to link back to individuals (research participants) and the research information they have supplied. In this case, opposed to pseudonymisation, there is no key file that would permit re-identification and this process is irreversible as all identifiers are removed. An example of anonymising names can be found in Figure 3. Figure 4 shows the comparison between pseudonymisation and anonymisation. Figure 3: Examples of anonymisation of names Figure 4: Comparison of pseudonymisation versus anonymisation 3.2.3 Data anonymisation process Data de-identification is an iterative process. Figure 5 below presents the four essential steps, considerations and actions necessary to ensure an effective and informed data de-identification process. Figure 5: Steps to de-identify your data Note: Adapted from UK Data Services (2015) and PDPC Singapore (2024) 1 Get to know your data Identify any direct and/or indirect identifiers in your dataset. See Chapter 1 for a detailed overview of identifiers. How to: Knowing your data involves answering questions, such as what type of data do you want to anonymise or pseudonymise (e.g., textual, numerical, audio, video)? What identifiers are present, such as direct, indirect, or strong indirect identifiers? 2 De-identify your data Select and apply anonymisation or pseudonymisation techniques to the data. See Chapter 3 for an overview of the techniques. Be consistent when applying the selected techniques throughout the dataset. Be aware that de-identifying your data can be a time-consuming or expensive process and should, therefore, be planned accordingly. How to: Ask yourself how you could de-identify the data while retaining its usefulness. Data usefulness refers to the data’s ability to address a range of research questions even after de-identification, while remaining rich in context. This will involve considering the data type, format, sensitivity, uniqueness, intended use, and reuse of the data. Finally, the de-identification process should be documented to ensure transparency. 3 Assess re-identification risks Re-identification risk assessment is the process of evaluating the likelihood that the de-identified data could still reveal individual identities. How to: Three techniques commonly used to assess re-identification risks are: • Singling Out: can an individual be isolated from the dataset? • Linking: can information within the dataset be combined with external data to identify an individual? • Inference: can details about an individual be inferred, guessed, or predicted based on patterns in the dataset or previous knowledge? Checking unique data recordsAre there any remaining sensitive or unique data points in the dataset? Check if any data points are unique in combination with other attributes. If a category created by any two attributes (e.g., age and occupation) has only one data point, that data point is unique and poses a re-identification risk. Aim to have at least three data points in each unique category to reduce this risk. Assessing sample vs population unique data recordsUniqueness in a dataset sample can still mean uniqueness in the broader population and an increased re-identification risk. For example, a rare disease patient or a specialized surgeon might be unique both in the dataset and in real life, making them identifiable. Investigating the existence of External databases or any other information: assess whether other datasets or any knowledge that could be used to re-identify your data exist. e-identification technology and skills: assess if (new) techniques and skills that could reverse anonymization or link data back to individuals exist. 1 2 3 4 Manage re-identification risk Full anonymisation is difficult, if not impossible, to achieve. Once data is made available, it can potentially be linked to external datasets, making it possible to re-identify individuals. Additionally, (new) techniques could emerge and enable re-identification. Therefore, when making data available, extra care should be taken to minimise re-identification risks. How to: •Specify access levels and conditions for the future user(s) who will access the data (e.g., a specific group or the general public). The broader the recipient range, the stricter the anonymisation measures should be. •Apply additional safeguard measures such as encryption, password protection, etc. •Ensure that, where necessary, legal agreements are arranged (e.g., data transfer or sharing agreements). 3.3 Practical Consideration Anonymization is not an absolute value, but a spectrum. The outcome of the anonymisation process can vary in degree of risk of re-identification. Some datasets can be fully anonymised while other datasets still have the risk of re-identification even after applying anonymisation techniques. In the following scenarios, the data cannot be considered fully anonymous as there is still the possibility of re-identification: o If the dataset still contains identifiable data or multiple identifiers that can be combined with additional information. o If the key files or metadata information could be used to re-identify individuals remain available (thus shared alongside the dataset elsewhere), the data cannot be deemed anonymised. The data is instead considered pseudonymised, which is still considered as personal data according to the GDPR. Balancing anonymization and data usefulness. Anonymisation can reduce data usefulness, potentially limiting its use in the original study in which it was collected and reuse in future studies. The more data is anonymised (by removing or altering personal identifiers), the less useful it may become for research. Therefore, sometimes personal details may need to be preserved, at least during the research, to maintain the data’s usefulness. For example, in a study examining the relationship between students’ performance and family socioeconomic status, key information such as grades and family income must be retained to answer the research question while personal details like names, gender, and age might be removed to protect privacy. Pseudonymization may be preferred over anonymization. anonymisation may not always be desirable, during and after research. For example, in longitudinal studies, tracking the same participants over time is crucial. In such cases, pseudonymisation is preferred over anonymisation, as it allows researchers to link data of the same participant. Similarly, for qualitive studies, pseudonymisation may be favoured to retain the context and richness of the data. Sharing anonymised data should be done with caution. There is always a risk of re-identification through singling out, inference, or linking with other data or techniques. Therefore, it is recommended these risks are carefully assessed before sharing data. If the risk of re-identification is high, the scope of data sharing should be limited, and conversely, if the risk is low, the scope of sharing can be broader. Documenting the anonymisation process. The process of anonymisation should be thoroughly documented for future reference, mainly for transparency but mostly to ensure data usability. Such documentation tracks all changes from the original data to anonymised data, including removals, replacements, aggregations, or generalisations. The anonymisation documentation should be stored securely and separately from the data files, as it may contain information that could enable re-identification. Fully anonymized data is not subject to GDPR Unlike pseudonymised data, fully anonymised data is not subject to GDPR regulations. However,full anonymisation is challenging to achieve. "],["textual.html", "4 Pseudonymisation and Anonymisation Techniques 4.1 Textual data", " 4 Pseudonymisation and Anonymisation Techniques Pseudonymisation and anonymisation techniques vary depending on the types and nature of the data. This guide will highlight various techniques for the following types of research data: textual data,numerical data, audio and visual data, and geospatial data. Most of the presented techniques are used for anonymisation. However, please note that if you apply an anonymisation technique, such as removing identifiable information, but retain this information in a separate key file, this is considered as pseudonymisation. 4.1 Textual data Textual data refers to any text-based content, such as transcripts of interviews, workshops and focus group discussions, open-ended questions in surveys, observational notes, meeting notes, etc. These types of data can be de-identified by generalising the identifying information, distorting or deleting the identifiers such as research participants’ names, gender, age, income, places and institutions names, addresses (physical location, email and IP), telephone numbers and any other sensitive information. The following techniques are commonly used to pseudonymise or anonymise textual data. Generalisation: Replace or aggregate Generalisation involves reducing detailed information or the precision of the information in a way that potentially disclosive information (identifiers) is replaced or aggregated. For example, instead of using a real name, an alias or general pronoun can be used to refer to a person. Examples of generalisation can be found in Figure 6 (next page). Distot In some instances, to preserve data usefulness, instead of generalising crucial information needed to understand the context, it is possible to distort the information by altering other identifiers. For example, you have interviewed someone who participated in a high-level sports competition. If that information is of importance in research, then you may change other personal or competition details. You could change the years when the competition took place, the region (place, country) or even the gender of the person if this is not crucial information. Suppression: Delete If the identifying information cannot be replaced or generalised, the entire variable or text may need to be deleted and explicitly marked as such by using [brackets]. See Figure 7 (next page) for examples of suppression. Masking Masking textual data involves replacing letters or symbols (punctuation) in identifying information entirely or partially with a special symbol (such as ’*’ or ‘x’). It is commonly used for identifiers such as email addresses, names of locations, etc. This technique is more suitable for structured tabular data, meaning that data is stored in the form of columns and rows. The technique can be applied manually, or using tools such as Excel, see more detailed in the masking numerical data in the section 3.2 Numerical data. For examples on masking textual data, see Figure 8. For more inspiration on how to anonymise textual data, please consult this illustrative example provided by CESSDA, which demonstrates the process of anonymizing an interview transcript. "],["numeric.html", "5 Numerical data", " 5 Numerical data Numerical data consist of numbers and is commonly organised in a table with rows and columns. Rows represent observations or attributes of each data subject, while columns display values for various variables or attributes of interest to researchers. Generalisation Generalisation techniques reduce the granularity of the attributes/records in the data. It can be performed by: **Categorising personal identifiers*. For example, age is replaced by an age category or range. Geographic units are formed by combining all ZIP codes with the same four initial digits, which contain more than 20,000 people. Top and bottom coding of the upper or lower ranges. Very high and low values are grouped into categories to minimise identifiability due to outliers. For example, ages above or below certain limits are classified as groups to prevent the identification of exceptionally older or younger individuals. A top code of“X or more”could be applied to avoid identifying older subjects. Collapsing and/or combining variables. Merging data recorded in two or more variables into a single category. This is particularly useful if the initial data collection creates several categories with very few data subjects in each. Perturbation can be used where small changes in value are acceptable. This technique involves rounding, adding noise, or replacing actual values with simulation values. It should not be used when data accuracy is critical. For examples on how to generalise (categorise and perturbate numerical data), see Figure 9. Rounding Rounding returns a number rounded to a base number to reduce precision. It can be used to deal with personal data, such as age, height and weight. Various tools can be used to apply the rounding technique, such as Excel, R, or Python. For instance, it is possible to use the MROUND function in Excel to round numerical identifiers. MROUND function will round the original value to a nearest value that is estimated by dividing the base number that is selected as parameter. For example, the function will turn the age of 44 to 40 when the base number is 20. Other practical examples of rounding are demonstrated in Figure 10. Noise addition Noise addition involves adding or subtracting the original values with a random number. This can be done manually or by means of tools such as Excel, R, or Python. The technique can be used for a few values in a dataset, such as outliers, or for the entire dataset. Large random values result in higher levels of noise. As a practical example, adding noise to the identifier “age” with R involves the following two steps: (1)generate a noise parameter using the sample(1:n) function to create a random number between 1 and n; (2)add or subtract this noise value from the original age. For instance, if the original age is 44 and the noise parameter is 5, the modified age becomes 49. See Figure 11 for additional examples of noise addition. Note that the results of noise addition are irreversible because each modification introduces randomness to the original values. Furthermore, when handling data with one or more continuous variables, the distribution of continuous variables should be preserved to maintain more information in the data. Suppression Suppression involves deleting identifying information from the data. Some attributes (variables) or records (observations) can be removed in some instances to make the identifier no longer unique. This is useful, for example, in the case of outliers that can easily allow the re-identification of an individual. Swapping The swapping technique is also known as shuffling or permutation to alter the association between the participants and their attributes. For instance, consider a dataset with 100 participants labelled as Participant 1 through Participant 100. Each participant has attributes such as income, education level, and job position. To reduce the risk of re-identification, attributes can be swapped between participants, for example exchanging the income of Participant 1 with that of Participant 50 and shuffle the job title for Participant 1 and Participant 87. By swapping the income and job title of participants, it reduces the likelihood of identifying an individual through both attributes. This process can be applied across the entire dataset, which effectively breaks the direct link between each participant and their original set of attributes. In this way, the distribution of the data would not be changed even though each participant’s attributes are altered. Note: This technique should not be used when the data has been collected to study relationships between attributes of the same research participants, such as the association between a job position and education level for each participant in the previous example. Masking numerical data Masking numerical data involves entirely or partially replacing digits with special symbols (such as * or x) in the same way words are masked in textual data as described in section 3.1. For instance, it is possible to use the REPLACE function in Excel to mask numerical identifiers such as phone number, income, email, IP address, etc. The REPLACE function requires three inputs: The cell containing the original value, identified by its column and row (e.g., A2). The starting and ending position of the digits/characters within all content stored in the cell to be replaced. The symbol used to replace the original digit(s)/character(s), such as * or x. As an example, to mask the first three digits of an income value ‘58435’ in cell A2 in Excel, the following formula can be used: Replace (A2, 1,3, *), which returns ’***35 ’. Additional examples of masking are shown in Figure 12. "],["audio_visual.html", "6 Audio-Visual data 6.1 Mute or bleep out the identifying information", " 6 Audio-Visual data Audio-visual data include various forms of multimedia content, such as video and audio recordings, including voices and visual components. In research, these types of data often capture interviews, workshops, focus groups, places, or any other form of communication with research participants involving their voices and/or images. This section outlines some of the techniques used to anonymise audio-visual data. Voice transformation/distortion This technique involves disguising voices such as altering the pitch in a recording. Keep in mind that transforming or distorting the voice can alter acoustic characteristics, which decreases the usefulness of the data. Therefore, the decision to modify sounds should be made based on your research objectives. Image blurring, pixelation or obscuring Video images or pictures of places or individuals can be blurred or pixilated to reduce the level of detail in an entire image or part of an image or picture, which makes it harder to identify a person, place or object, etc. This technique involves reducing the level of detail in entire or part of images, making it harder to identify for example a person, location, licence plate, private spaces etc. For instance, you can pixelate the face of a person. See Figure 13 for an example. Figure 13: Example of a pixelated face. (Derived from pexels. CC0 by Pixabay) Simpel distortion of the audio or imagery can only prevent idetnfication based on the speaker’s voice or iamge but it cannot remove other idetnfiying information that might exist in the recoding, such as speaking patterns or the mentioned personal information. 6.1 Mute or bleep out the identifying information In audio or video recordings, you can also mute, remove or bleep out identifying information to anonymise the audio-visual data. For example, you can mute, remove or bleep out identifiers such as names, places, or other information. Please see Figure 14 below, where the personal name has been muted in the audio recording. Figure 14. Demonstration of removing a part of an audio recording. Software that can be used for audio-visual data There are various software and tools that can be used to anonymise audio visual data. The following are some examples: • For audio data: audio editing software such as Audacity can be used to transform or distort the voice or bleep out personal information. • For video data: video editing software such as Adobe Premiere can be used to blur or pixelate video recordings or transform, distort or bleep out information in the audio recording. • For images: image editor tools such as Adobe Photoshop and Paint can be used to blur, pixelate or obscure faces or personal information in pictures and images. "],["geospatial.html", "7 Geospatial data", " 7 Geospatial data This section presents examples of the techniques commonly used for de-identifying geospatial data. Geospatial data refer to information linked to specific geographic locations. Geospatial data are often used in research for analysis, visualisation, and understanding of relationships and patterns within a geographic context. Geospatial data anonymisation involves techniques that reduce spatial or temporal resolution, i.e., the precision or scale at which data is captured (e.g., exact GPS coordinates vs. general area), and granularity, i.e., the level of detail in data attributes (e.g., full address vs. city name). Commonly used techniques to anonymise geospatial data include spatial aggregation, attribute anonymisation, and spatial displacement. Spatial aggregation The aggregation technique helps mask the exact locations while preserving overall spatial patterns and trends. It can take two forms: The first form is known as area aggregation, which involves reducing the level of detail by summarising spatial details (the location of a household or a hospital) into larger spatial units, such as zip codes, cities, census blocks or any other large administration or geographic units. Figure 15: Spatial aggregation of individual cases using census administrative units (#fig:geo_data) Spatial aggregation of individual cases using census enumeration units (Zandbergen, 2014). Figure from “Ensuring confidentiality of geocoded health data: Assessing geographic masking strategies for individual‐level data,” 2014, by P.A. Zandbergen, Advances in medicine, 567049. CC-BY 3.0 The second form is point aggregation, which is assigning multiple individual records to one geographic coordinate. Examples include a population dot map where one dot represents 100 persons (see Figure 15). Attributes anonymisation The technique involves deleting or modifying any attribute or variable that could identify or link to individuals. Attributes such as the coordinates of households, exact locations of rare diseases, cadastral boundaries, ages, income, crime incidences, etc. can be deleted or modified similarly to numerical or textual data, as described in the previous sections. Spatial displacement/adjusting spatial coordinates Spatial displacement is a technique that involves shifting spatial data points (coordinates) to a different location within a specific range or distance by, for example, adding or subtracting a fixed or random number to both the x-axis and the y-axis. Spatial displacement preserves the spatial distribution and density of your data while making it difficult to pinpoint the exact original location of each point. It involves altering data accuracy to weaken links between the data and the individuals. This would mean that a map feature, for example, a point, is displaced to a new location (d-distance) away from its original location. Figure 16: The original coordinates of the University of Twente on Google Maps (52.24001648578193, 6.849739376582042) are displaced to new random coordinates (52.245130, 6.837690) at a given distance Figure 7.1: Spatial aggregation of individual cases using census enumeration units (Zandbergen, 2014). "],["resources.html", "8 Resources", " 8 Resources The development of this guide was informed and inspired by a broad range of resources on data anonymisation, pseudonymisation, privacy, and research ethics. The references listed below constitute the key sources that significantly informed the conceptualisation, structure, and content of this guide: BMS Datalab University of Twente. Guidelines Personal Information. https://www.utwente.nl/en/bms/datalab/research-data-and-gdpr/guidelinespersonal/ General Data Protection Regulation (2016). Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA relevance). https://eur-lex.europa.eu/eli/reg/2016/679/oj Finnish Social Science Data Archive (FSD). Anonymisation and Personal Data. https://www.fsd.tuni.fi/en/services/data-management-guidelines/anonymisation-and-identifiers/ Personal Data Protection Commission (PDPC) (2024). Guide to Basic Anonymisation. https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/advisory-guidelines/guide-to-basic-anonymisation-(updated-24-july-2024).pdf Research Data Management Support, Huijser, D., Moopen, N., Flores-Dourojeanni, J., Beltrán, M., Bruijn, K. de ., Bruin, J. de ., Capel, D., Dijkstra, F., Einarson, S., Folkers, J., Franzke, A., Graaf, J. de ., Hout, S. van . den ., Huigen, F., Janssen, R. D. T., Jovic, K., Kessels, L., Kleerebezem, S., … Weijdema, F. (2025). Data Privacy Handbook (v2025.05.06) [Computer software]. Zenodo. https://doi.org/10.5281/zenodo.15350653 UK Data Service. (2025). Research data management. https://ukdataservice.ac.uk/learning-hub/research-data-management/#anonymisation Zandbergen, P. A. (2014). Ensuring confidentiality of geocoded health data: Assessing geographic masking strategies for individual‐level data. Advances in Medicine, 2014, 567049. https://doi.org/10.1155/2014/567049 "],["glossary.html", "9 Glossary", " 9 Glossary Anonymisation: a de-identification process which involves permanently deleting direct and/or indirect identifiers from the data, such that there is no way to link back to individuals (research participants). Biometric identifiers: unique and often permanent traits of an individual, which could enable the identification of the individual. Biometric identifiers include physiological and behavioural characteristics, such as facial features, fingerprints, DNA, voice, and gait. They also include less distinctive traits that may change over time, such as visible marks (e.g., tattoos or scars). De-identification: the process of minimising personal information in the dataset to reduce the risk of tracking back to a living human. It is an umbrella term used to refer to either anonymisation or pseudonymisation. Data minimisation: a fundamental principle of data privacy and protection which requires limiting personal data collection and retention to only what is relevant, necessary, and adequate to accomplish a given research purpose. Data type: refers to the structure of data; it can include textual, numerical, audio-visual, geospatial data and more. Data usefulness: the extent to which the data can effectively answer a variety of research questions, both within and beyond the original study scope. Usefulness can be retained even after the data have been de-identified, provided that there are sufficient details and context information. De-identification documentation: records about the process of anonymisation and/or pseudonymisation, including details of all modifications from the original data to de-identified data. Direct identifiers: any piece of information unique to an individual and sufficient on its own to identify the individual. Distortion: altering personal information in data. Encryption: the process of securing personal data by converting it into a coded format, making it accessible only to those with the encryption key. General Data Protection Regulation (GDPR): A regulation by the European Union (EU) that governs the processing of personal data, meaning how personal data of individuals are, for example, collected, used, protected etc. The GDPR is applicable in the European Economic Area (EEA). The GDPR is also referred to as the European privacy law. Generalisation: reducing detailed information or the precision of the information in a way that potentially disclosive information (identifiers) is replaced or aggregated. Indirect identifiers: any information that can reveal an individual’s identity if combined with additional available information. Key file: a file that contains links between participants’ identifying information and pseudonyms (substitute values). Masking: replacing characters, digits, or symbols (punctuation) with special symbols in identifying information entirely or partially with a special symbol (such as ’*’ or ‘x’). Muting or bleeping out: mute, remove or bleep out identifying information to anonymise audio-visual data. Open-ended questions: questions that solicit a more detailed, thoughtful, and descriptive response as opposed to closed questions. Personal data: any information that can be used to directly or indirectly identify a living individual. Perturbation: to alter small changes in value and the association between research participants and their attributes. Pseudonymisation: pseudonymisation is a de-identification process which involves creating pseudonyms. Pseudonymised data can no longer be linked to a specific individual without additional information (key file). Re-identification risk assessment: evaluating the likelihood that the de-identified data could still reveal individual identities. Research participants: individuals who take part in research projects and provide their responses or data for the study. Rounding: adjusting a number to the nearest multiple of the base number in the dataset, effectively grouping the data point into the closest category defined by that base. Special categories of personal data: also known as sensitive personal data. Any information about a person’s racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health, sex life, or sexual orientation. Strong indirect identifiers: identifiers that can significantly increase the risk of easily identifying an individual when combined with other information. Suppression or deletion: deleting identifying information from the data, leaving the space empty or explicitly marking as such by using brackets [……]. Swapping: also known as shuffling or permutation. The technique of altering the association between the participants and their attributes by exchanging the attributes’ values between participants. "],["attribution.html", "10 Attribution", " 10 Attribution This “Research Data Pseudonymisation and Anonymisation: A Practical Guide” was created by (in alphabetical order): Minsi Li, University of Twente m.li-2@utwente.nl 0000-0002-9719-239X Deniece S. Nazareth, University of Twente d.s.nazareth@utwente.nl 0000-0003-0555-4101 Alice Nikuze, University of Twente a.nikuze@utwente.nl 0000-0002-1849-9968 We are grateful to our former colleague, Qian Zhang, for her invaluable contribution to the conceptualisation of this guide. Her ideas and vision laid the foundation for its contents. We would also like to thank Victor van Wanningen and Maryam Zavichi Tork (University of Twente) for their valuable feedback on the draft version of this guide. We would like to express our appreciation for the language editing support provided by our colleague at the Language Centre of the University of Twente. This work can be cited as: Li, M., Nazareth, D.S., &amp; Nikuze, A. (2025). Research Data Pseudonymisation and Anonymisation: A Practical Guide (1.0). University of Twente. https://doi.org/10.5281/zenodo.17492062 "]]
