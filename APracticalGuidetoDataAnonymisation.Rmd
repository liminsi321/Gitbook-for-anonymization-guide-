--- 
title: "A Practical Guide to Psedunymization and Anonymization of Research Data"
author: "Alice Nikuze, Deniece  Nazareth, Minsi Li"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: "https://github.com/UniversityOfTwente/anonymization-guide/tree/main/_book"
github_repo: "UniversityOfTwente/anonymization-guide"
description: |
  This is a practical guideline for anonymization of reserach data, created by Alice Nikuze, Deniece  Nazareth, Minsi Liat University of Twente.  
favicon: figure/favicon.ico
apple-touch-icon: "figure/logo.jpeg"
apple-touch-icon-size: 180
output:
  bookdown::pdf_book:  default      # PDF output
    #latex_engine: xelatex    # You can use pdflatex or lualatex as well
  bookdown::html_book:       # HTML output
    toc: true
    toc_float: true
    config:
      html:
        code_folding: 
        
        
        
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.align = "center")
Sys.setlocale("LC_TIME", "English")

```



# Introduction
```{r, results='asis',echo=FALSE}
# Load required packages
#install.packages("renv")
#library(renv)
#renv::init()
#install.packages("kableExtra")
#install.packages("readxl")
library('bookdown')
library('DiagrammeR')
library('ggplot2')
library('bookdown')
library('rmarkdown')
library('flextable')
library('kableExtra')
library('readxl')
#install.packages("tinytex")
#tinytex::install_tinytex(force = TRUE)
```

In today's data-driven world, privacy concerns have become paramount. With the increasing collection of personal data in research, researchers bear the responsibility of ensuring the protection of the privacy rights of the individuals participating in their studies. More specifically, the General Data Protection Regulation (GDPR) in Europe requires researchers affiliated with any institution within the European Economic Area (EEA) to protect the privacy of research participants if they handle personal data in their research. In this context, the more personal data are collected, the greater the need for methods for data de-identification.  

This guide provides practical guidance on two methods of data de-identification, pseudonymizing and anonymizing data. It includes hands-on examples of various techniques for pseudonymising and anonymising various types of research data.  

The rest of the guide is structured as follows: 


<div class="pink-box">
•	**Chapter 2** defines personal data and explains different types of identifiers.

•	**Chapter 3** introduces fundamental principles and concepts of data de-identification. The chapter concludes with practical considerations and best practices.  

•	**Chapter 4** delves into the techniques of pseudonymization and anonymisation for textual, numerical, audio-visual and spatial research data. Additionally, practical examples are included.

</div>

Although extensive techniques and examples are given, please note that this guide does not cover every issue related to pseudonymization or anonymisation. Furthermore, the de-identification of research data is often complex and requires a case-by-case solution. Therefore, please always consult your privacy contact person for more information.  

**This guide has been written by:**  
Alice Nikuze, University of Twente 

Deniece Nazareth, University of Twente 

Minsi Li, University of Twente

The authors would like to extend our sincere gratitude to our former colleague, Dr Qian Zhang, for her immense dedication and contribution during the conceptualization phase of this guide. We also wish to express our sincere appreciation to Nestor De la Paz Ruiz for his contribution, whose efforts were instrumental in transforming this guide into a Gitbook. 

**This work is licensed under a Creative Commons Attribution DOI: xxxxxxxxx** 

<!--chapter:end:index.Rmd-->

# What is personal data? {#personal_data}

The term ‘’Personal data” refers to any information that enables you to identify a living person. According to the GDPR, a living person can be identified, directly or indirectly, by means of an identifier such as a name, a citizen identification number, location data, or by other information specific to the physical, physiological, genetic, mental, economic, cultural, or social identity of that living person. 

## Categories of identifiers

There are three categories of identifiers: 


| **Direct identifiers**           | **Indirect identifiers**    |**Strong indirect identifiers**        |
|--------------------------------------------------------------------------|-------------------------------------------------------------------|-----------------------------------------------------------------------------|
|Any piece of information unique to an individual and sufficient on its own to identify that individual, such as a name, a phone number, a picture, security numbers, audio/sound, video, photographs, etc. | Any information that can reveal someone's identity when combined with other available information. Examples include an address, age, gender, occupation, location, health-related records, ethnic group, etc.          | Some identifiers are particularly considered strong indirect identifiers because they can significantly increase the risk of easily identifying an individual when combined with other information. Examples of strong identifiers include a rare event, a rare disease, an unusual job title, a date of birth, an IP address, etc. |{.gradient-table}


<div class="three-column-container">
  <div style="text-align: center" class="three-column"> >**Direct identifiers**<br>   
    Any piece of information unique to an individual and sufficient on its own to identify that individual, such as a name, a phone number, a picture, security numbers, audio/sound, video, photographs, etc.
  </div>
  <div class="three-column"> >**Strong indirect identifiers**<br>  
    Any information that can reveal someone's identity when combined with other available information. Examples include an address, age, gender, occupation, location, health-related records, ethnic group, etc.  
  </div>
  <div class="three-column"> >**Indirect identifiers**<br>  
    Some identifiers are particularly considered strong indirect identifiers because they can significantly increase the risk of easily identifying an individual when combined with other information. Examples of strong identifiers include a rare event, a rare disease, an unusual job title, a date of birth, an IP address, etc. 
  </div>
</div>


For extensive overview of different types of identifiers, see Table 1. 

**Table 1.** Examples of identifiers typically processed in research  @Finnish_Social_Science_Data_Archive


```{r type_identifiers,fig.label="type_identifiers",fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/type_identifiers.png",dpi = 600) 
```


## Sensitive personal data

This is a special category of personal data”. It includes any information about a person’s racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health, sex life, or sexual orientation information.

Sensitive personal information can put individuals or certain groups in vulnerable positions, and their misuse can potentially lead to physical or mental harm, financial loss, political or religious persecution, disclosure of sexual orientation discrimination, or embarrassment. Therefore, handling such data requires extra caution.

<!--chapter:end:01-intro.Rmd-->

# Principles, concepts and practical considerations {#minimization}

## Data minimization

*Data minimization* is a fundamental principle of data privacy, as emphasized under the GDPR. It requires limiting data collection and retention to only what is **relevant**, **necessary**, and **adequate** to accomplish a given purpose. 

Collecting only the minimum amount of personal data necessary for your research reduces the risks associated with excessive data collection. More personal data implies bigger risks of data breaches and their misuse, thereby increasing potential consequences for the research participants.

In line with this principle, a researcher should only collect and retain the personal data that is required for the intended research study. Personal data minimization in research can be achieved by:


<ul>
  <li><details><summary><b>**Carefully planning what personal data is necessary for your research purpose. **</b></summary>
  <div>Plan what specific personal data is truly needed early in the research design phase. For example, avoid collecting participants' ages if your study does not examine age-related effects.<br></div>
  </details></li>
  <li><details><summary><b>**Collecting aggregated information when detailed information is not required. ** </b></summary>
  <div>If broader categories (e.g., age ranges) suffice for your analysis, there's no need to collect exact ages.<br></div>
  </details></li>
  <li><details><summary><b>**Limiting open-ended questions in surveys.**</b></summary>
  <div>Open-ended responses occasionally include personal data about the respondents themselves or others. Reducing the number of open-ended questions can minimize the risks of collecting unnecessary personal data.</div></details></li>
  <li><details><summary><b>	**Instructing research participants sufficiently to avoid unnecessary personal data.** </b></summary>
  <div>Inform participants ahead of the survey and interview to refrain from sharing unnecessary personal information, such as names or addresses of others.  Adding clear instructions and reminding the participants not to mention personal data (e.g., other people's names or addresses) is well worth the effort and can prevent unintentional personal data collection.</div></details></li>
    <li><details><summary><b>	**Adjusting the settings of the data collection tool (e.g., survey tools) to minimize personal data collection. ** </b></summary>
  <div>Review and configure tools like survey platforms to prevent the default collection of unnecessary personal data, such as location or IP addresses. Always check and adjust settings to align with the data minimization principle. When data collection is finalized, please migrate personal data from the data collection tools to safe data storage as soon as possible. 
  </div></details></li>

</ul>


## Pseudonymisation versus Anonymisation  


When personal data are necessary and have been collected for research, research participants’ identities should be protected. Two techniques- **pseudonymisation** and **anonymization** are used to ensure that research participants remain anonymous and untraceable during and after research. 


**Pseudonymisation**

*Pseudonymisation* is a de-identification process after which personal data can no longer be attributed to a specific data subject without the use of additional information. This process is also referred to as coding. It involves creating <u> **two separate files**</u>, one linking the personal data to the pseudonyms (made-up values/tokens) and another file which contains only the pseudonyms and the supplied research information. The former file is known as **a key file** or **code list** and must be kept separately from the latter file. 

The key file should be securely stored and protected with measures such as password protection and encryption to prevent the re-identification of research participants.

```{r pseudo-key,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/pseudo-key.png") 
```

**Anonymisation**

*Anonymisation* is another de-identification process which involves permanently deleting direct and/or indirect identifiers from the data, such that there is no way to link back to individuals (research participants) and the research information they have supplied. In this case, there is <u>no key file</u> that would permit re-identification as this process is irreversible. 



```{r anonymization_example,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/anonymization_example.png") 
```

Figure XX. Anonymization of names

Pseudonymization differs from anonymization as it is a reversible process due to the key file and re-identification is possible. Anonymization is irreversible as all identifiers are removed and there is no key file (see Figure XX) thus re-identification is not possible.




:::{.bubble}

Pseudonymization differs from anonymization as it is a reversible process due to the key file and re-identification is possible. Anonymization is irreversible as all identifiers are removed and there is no key file (see Figure XX) thus re-identification is not possible.
:::



```{r comparison_pseu_anony,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/comparison_pseu_anony.png") 
```

## Data anonymisation process

Data anonymisation is an iterative process.  The figure below presents the important steps, considerations and actions necessary to ensure an effective and informed data anonymisation process.


The setps for anonymization can be found in the table below @UK_Data_Service,@PDPC. 
```{r anonymization_steps,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/anonymization_steps.png") 
``` 
Figure x. Steps to de-identify your data. (Note: Adapted from UK Data Service and PDPC Singapore)


**1. Get to know your data**

Identify any direct and/or indirect identifiers in your dataset. See Chapter 1 for a detailed overview of identifiers. 

***How to:*** 

Knowing your data involves answering questions such as: what type of data do you want to anonymize or pseudonymize (e.g., textual, numerical, audio, video)? What identifiers are present, such as direct, strong, indirect, or weak indirect identifiers?

**2.De-identify your data**

Select and apply anonymization or pseudonymization techniques to the data. See Chapter 3 for an overview of the techniques. Be consistent when applying the selected techniques throughout the dataset.

***How to:*** 

Ask yourself how you could de-identify the data while retaining its usefulness. This will involve considering the data type, format, sensitivity, uniqueness, intended use and reuse of the data. Finally, the de-identification process should be documented to ensure transparency.


**3. Assess re-identification risks** 

Identify any direct and/or indirect identifiers in your dataset. See Chapter 1 for a detailed overview of identifiers. 

***How to:*** 

Three techniques commonly used to assess re-identification risks are:

•	Singling Out: can an individual be isolated from the dataset?
•	Linking: can information within the dataset be combined with external data to identify an individual?
•	Inference: can details about an individual be inferred, guessed, or predicted based on patterns in the dataset or previous knowledge

Therefore, assessing re-identification can involve reviewing the data to confirm that no personal data or significant re-identification risks remain by:

<ul>
  <li><details><summary><b>**Checking unique data records**</b></summary>
  <div>Are there any remaining sensitive or unique data points in the dataset? Check if any data points are unique in combination with other attributes. If a category created by any two attributes (e.g., age and occupation) has only one data point, that data point is unique and poses a re-identification risk. Aim to have at least three data points in each unique category to reduce this risk.<br></div>
  </details></li>
  <li><details><summary><b>**Assessing sample vs population unique data records:** </b></summary>
  <div>Uniqueness in a dataset sample can still mean uniqueness in the broader population and an increased re-identification risk. For example, a rare disease patient or a specialized surgeon might be unique both in the dataset and in real life, making them identifiable.<br></div>
  </details></li>
  <li><details><summary><b>**Investigating the existence of:**</b></summary>
  <div>o	**External databases or any other information**: assess whether other datasets or any knowledge that could be used to re-identify your data exist.
  
  o	**Re-identification technology and skills**: assess if (new) techniques and skills that could reverse anonymization or link data back to individuals exist.
  </div></details></li>
</ul>


**4.	Manage re-identification risk. **

Full anonymization is difficult, if not impossible, to achieve. Once data is made available, it can potentially be linked to external datasets, making it possible to re-identify individuals. Additionally, various (new)new techniques could emerge and enable re-identification. Therefore, when making data available, extra care should be taken to minimize re-identification risks.


***How to:*** 

•	Specify access levels and conditions for who will access the data (e.g., a specific group or the general public). The broader the recipient range, the stricter the anonymization measures should be. 

•	Apply additional safeguard measures such as encryption, password protection, etc.

•	Ensure that, where necessary, legal agreements are arranged (e.g. data transfer or sharing agreements).


## Practical Consideration
<ul>
  <li><details><summary><b>**Anonymization is not an absolute value, but a spectrum.**</b></summary>
  <div>In the following scenarios, data cannot be considered anonymized:
  
o	If the dataset still contains identifiable data. 

o	If the key files or any other information that could be used to re-identify individuals remain available (shared alongside the dataset orelsewhere), the data cannot be deemed anonymised. The data is instead considered **pseudonymized**. 
<br></div>
  </details></li>
  <li><details><summary><b>**Balancing anonymization and data usefulness.** </b></summary>
  <div>Anonymization can reduce data utility, potentially limiting its use and reuse in future studies. The more the data is anonymised (by removing or altering personal identifiers), the less useful it may become for research. Therefore, sometimes personal details may need to be preserved, at least during the research, to maintain the data’s usefulness. For example, in a study examining the relationship between students' performance and family socioeconomic status, key information such as grades and family income must be retained to answer the research question while personal details like names, gender, and age might be removed to protect privacy. <br></div>
  </details></li>
  <li><details><summary><b>**Pseudonymization may be preferred over anonymization.**</b></summary>
  <div>Anonymization may not always be desirable, during and after research. For example, in longitudinal studies, tracking the same participants over time is crucial. In such cases, **pseudonymization** is preferred over anonymization, as it allows researchers to link data across multiple different time periods. Similarly, for qualitive studies, pseudonymization may be preferred over anonymization to retain the context and richness of the data. </div></details></li>
  
  <li><details><summary><b>	**Sharing anonymised data.** </b></summary>
  <div>There is always a risk of data re-identification through singling out, inference, or linking with other data or techniques. Therefore, it is recommended to carefully assess these risks before sharing data. If the risk of re-identification is high, the scope of data sharing should be limited, and conversely, if the risk is low, the scope of sharing can be broader.</div></details></li>
    
  <li><details><summary><b>	**Documenting the anonymization process.** </b></summary>
  <div>Anonymization process should be thoroughly documented for future reference, mainly for transparency but mostly to ensure data usability. Such documentation tracks all changes, including removals, replacements, aggregations, or generalizations. The anonymisation documentation should be stored securely and separately from the data files, as it may contain information that could enable re-identification. </div></details></li>
    <li><details><summary><b>	**Fully anonymized data is not subject to GDPR** </b></summary>
  <div>Unlike pseudonymized data, fully anonymized data is not subject to GDPR regulations. However, it is worth noting that **full anonymization is challenging to achieve**. </div></details></li>
  
</ul>

<!--chapter:end:02-personal_data.Rmd-->

# Pseudonymization and Anonymization Techniques 

Pseudonymisation and anonymisation techniques vary depending on the types and nature of the data. This guide will discuss the techniques for the following types of research data: **textual data**,**numerical data**, **audio and visual data**, and **geospatial data**.


## Textual data

Textual data refers to any text-based content such as transcripts of interviews, workshops and focus group discussions, open-ended questions in surveys, observational notes, meeting notes, etc. This data type can be de-identified by generalizing the identifying information, distorting or deleting the identifiers such as research participants' names, gender, age, income, places and institutions names, addresses (physical location, email and IP), telephone numbers and any other sensitive information. The following techniques are commonly used to pseudonymize or anonymize textual data.


### Generalisation: Replace or aggregate

Generalization involves reducing detailed information or the precision of the information in a way that potentially disclosive information is replaced or aggregated.

This approach mainly involves replacing or aggregating identifiers with vague descriptors, such as replacing the actual names of individuals with aliases or with other aggregated information.



```{r textual_generalization,fig.align='center',fig.width=10, fig.height=7, out.width="98%", echo=FALSE}
  knitr::include_graphics("figure/textual_generalization.png") 
```



### Distort

In certain cases, instead of generalizing crucial information needed to understand the context and content, it's possible to distort the information by changing other identifiers.  
For example, if you have interviewed someone who participated in a high-level sports competition, and if that information is of importance in research, then you may change other values. You could change the years when the competition took place, the region (place, country) or even the gender of the person if this is not crucial information. 


### Suppression: Delete 
 
If the identifying information cannot be replaced or generalized, the entire variable or text may need to be deleted and explicitly marked as such by using [brackets].  


```{r anony_vis1,fig.align='center',fig.width=12, fig.height=9, out.width="95%",echo=FALSE}
  knitr::include_graphics("figure/anonymization techniques_surpression_300.jpg") 
```

### Masking 

Masking textual data involves replacing letters or symbols (punctuation) in identifying information entirely or partially with a special symbol (such as '*' or 'x'). It is commonly used for identifiers such as email addresses, names of locations, etc.  

This technique is more suitable for structured tabular data, meaning that data is stored in the form of columns and rows.


```{r mask_textual,fig.align='center',fig.width=10, fig.height=7, out.width="90%", echo=FALSE}
  knitr::include_graphics("figure/textual_mask.png") 
```


For more inspiration on how to anonymize textual data, please consult this illustrative example provided by [CESSDA](https://dmeg.cessda.eu/Data-Management-Expert-Guide/5.-Protect/Anonymisation), which demonstrates the process of anonymizing an **interview transcript**. 




<!--chapter:end:03-textual.Rmd-->

# Numerical data

Numerical data is data in the form of numbers and is often displayed in a table with rows and columns. The techniques commonly used to anonymise numerical data include:

## Generalisation

 Generalisation techniques reduce the granularity of the attributes/records in the data. It can be performed by：
 
(1) **Categorising personal identifiers.** For example, age is replaced by an age category or range.

(2) **Top and bottom coding of the upper or lower ranges.** Very high and low values are grouped into categories to minimize identifiability due to outliers. For example, age records above a certain upper and lower limit are classified as groups to avoid recognition of exceptionally older or younger individuals while preserving actual ages for the rest of the population within the wider group researched. A top code of “ X or more” could be applied to avoid identifying older subjects. 

(3) **Collapsing and/or combining variables.** Merging data recorded in two or more variables into a single category. This is particularly useful if the initial data collection creates several categories with very few subjects in each.

(4) **Perturbation can be used where small changes in value are acceptable.** This technique involves rounding, adding noise, or replacing real values with simulation values. It should not be used when data accuracy is critical. The following section demonstrates examples of rounding and adding noise techniques with practical examples.



**Rounding**

It returns a number rounded to a base number to prevent exact matching with external data sources (https://sdcpractice.readthedocs.io/en/latest/anon_methods.html#special-case-census-data). It can be used to deal with data like age, height, weight, or house number. Examples of rounding is demonstrated with tools, Excel, R, or Python.

```{r rounding,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/rounding.png") 
```
**Noise addition**

This technique adds or subtracts the original values with a random number. Larger random numbers result in higher levels of noise. Note that the results of noise addition are irreversible because each modification introduces randomness to the original values.

This technique can be used to few values, such as outliers, or to entire dataset. When handling data with continuous value, the distribution of continuous data should be preserved to maintain more information in the datasets. Examples can be found in the table below.

```{r noise_addition,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/noise_addition.png") 
```



## Suppression

Suppression involves deleting the identifying information from the data. Some attributes (variables) or records (observations) can be removed in certain cases to make the identifier is not unique anymore. 

## Swapping
This technique is also known as shuffling and permutation.

When each data subjects has attributes which are stored in the columns in a spreadsheet, swap the attributes among two random data subjects till exhaust all data subjects in the dataset. In this way, the distribution of attributes would not be changes, while each data subject’s attributes are changed. For example, when collecting each data subject’s email address and IP address, swap each subject’s IP address would reduce the chances of identifying one data subject with their both email address and IP address. 

## Masking

The same way letters in textual data are replaced, numbers can be entirely or partially replaced with a special symbol (such as '*' or 'x').

See the table below for examples.

```{r mask_numeric,fig.align='center',fig.width=10, fig.height=7, out.width="90%", echo=FALSE}
  knitr::include_graphics("figure/mask_numeric.png") 
```


<!--chapter:end:04-numeric.Rmd-->

# Audio-Visual data 

Audio-visual data includes various forms of multimedia content, such as videos, movies, and audio recordings, including voices and visual components. In research, these forms of data capture interviews, workshops and any other form of communication with research participants involving their voices and/or images. Anonymizing these types of data involves modifying the audio, video or images, which will be introduced in this section.


<div class="attention">
Keep in mind that these techniques can be time-consuming or expensive to use.</div>
<div> Icons made by <a href="https://www.flaticon.com/authors/icon-mania" title="Icon mania"> Icon mania </a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com'</a></div>


## Voice transformation/distortion

This technique involves disguising voices such as altering the pitch in a recording. Keep in mind that transforming or distorting the voice can reduce the usefulness of the data. Therefore, the decision of modifying sounds should be made based on your research objectives.

## Blurring, pixelation or obscuring

Blurring by pixelating specific regions of a video image or a picture, such as faces, can effectively anonymize individuals. This technique involves reducing the level of detail in those areas, making it harder to identify the person. In addition, you can also obscure the face of a person. See Image XX for an example of pixelating a face.

```{r blurred_face,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/blurred_face.jpg") 
```

<div class="bubble">
**Simpel distortion** of the audio or imagery can **only prevent idetnfication** based on the speaker's voice or iamge but it **cannot remove other idetnfiying information** that might exist in the recoding, such as speaking patterns or the mentioned personal information.

</div>

## Mute or bleep out the identifying information

In audio or video recordings, you can also mute or bleep out identifying information to anonymize the audio-visual data. For example, you can mute or bleep out identifiers such as names, places or other information. Please see image XX below where the personal name has been muted in the audio recording.

```{r mute,fig.align='center',fig.width=10, fig.height=7, out.width="80%", echo=FALSE}
  knitr::include_graphics("figure/mute.png") 
```
<div class="software">
**Software for audio-visual data**

  •	For audio data, audio editing software such as Audacity can be used to transform or distort the voice or bleep out personal information.

  •	Different video editing software such as Adobe Premiere can be used to blur or pixelate video recordings or transform, distort or bleep out information in the audio recording.

  •	For images, image editor tools such as Photoshop and Paint can be used to blur, pixelate or obscure faces or personal information in pictures and images.

</div>




<!--chapter:end:05-audio_video.Rmd-->

# Geospatial data

Geospatial data refers to data that is associated with specific geographic locations.  Spatial data are used in research for analysis, visualization, and understanding of relationships and patterns within a geographic context. This section presents some of the techniques used to de-identify spatial data. The existing anonymisation techniques fall into two categories of *generalization* and *randomization*.

## Spatial generalisation

**Spatial aggregation**

Spatial aggregation is on the techniques commonly used to anonymise geospatial data. The method helps to mask the exact locations of individuals while still preserving the overall spatial patterns and trends. Spatiall aggregation can take two forms.  The first form, which is area aggregation, involves reducing the level of detail by summarising the spatial details into larger spatial units, such as census blocks, zip codes, cities or any other administrative units. 
The second form is known as point aggregation and involves assigning multiple individual records to one point location. Examples include a population dot map where one dot represents 100 persons.  


```{r geo_data,fig.align='center',fig.width=10, fig.height=7, out.width="80%",fig.cap="Spatial aggregation of individual cases using census enumeration units (Zandbergen, 2014).", echo=FALSE}
  knitr::include_graphics("figure/geo_data.png") 
```

**Spatial displacement/Adjusting Spatial Coordinates**

Spatial displacement is another simplest way of anonymising data with exact spatial coordinates by displacing the coordinates in some way.  The technique involves shifting spatial data points to different locations within a certain range or distance by for example, adding or subtracting a fixed or random number to both the x-axis and the y-axis. 

This can preserve the spatial distribution and density of your data while making it difficult to pinpoint the exact location of each point. It involves altering data accuracy to weaken links between the data and the individuals. This would mean that a map feature, for example, a point, is displaced to a new location (d – distance) away from its original location. 

Spatial displacement should be done with utmost care to ensure that the adjustments do not conflict with the spatial realities, for example by putting a house in the middle of the sea!

## Attribute anonymisation

Attribute anonymisation involves deleting or modifying any attributes or variables that could identify or link to data subjects, such as names, addresses, phone numbers, or email addresses. This technique can protect the personal or confidential information of data subjects while retaining some non-identifying information for analysis. 

<!--chapter:end:06-geospatial.Rmd-->

# References

<!--chapter:end:07-reference.Rmd-->

